{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c62bc7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:11:53.898950Z",
     "start_time": "2022-12-21T10:11:42.488738Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kintal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there, how are you doing today?', 'The weather is great today.', 'The sky is blue.', 'python is awesome']\n",
      "['Hello', 'there', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'today', '.', 'The', 'sky', 'is', 'blue', '.', 'python', 'is', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "# tokenize input\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "example_text = \"Hello there, how are you doing today? The weather is great today. The sky is blue. python is awesome\"\n",
    "print(sent_tokenize(example_text))\n",
    "print(word_tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54d52ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:14:04.207749Z",
     "start_time": "2022-12-21T10:14:03.322818Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kintal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# show stop words\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b049f7b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:20:48.634211Z",
     "start_time": "2022-12-21T10:20:48.610768Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good', 'boy', '.', 'good', 'coding']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete stop words from input\n",
    "text = 'he is a good boy. he is very good in coding'\n",
    "text = word_tokenize(text)\n",
    "text_with_no_stopwords = [word for word in text if word not in stopwords.words('english')]\n",
    "text_with_no_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c6c6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:22:58.004062Z",
     "start_time": "2022-12-21T10:22:57.988000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn\n",
      "earn\n",
      "earn\n",
      "earn\n"
     ]
    }
   ],
   "source": [
    "# find words stems\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()    ## создаём объект для PorterStemmer\n",
    "example_words = ['earn', 'earning', 'earned', 'earns']  ##слова для примера\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))   ##выделяем корни слов, используя ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6349257d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:26:59.484669Z",
     "start_time": "2022-12-21T10:26:40.582954Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kintal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history\n",
      "formality\n",
      "change\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer() ## создаём объект для WordNetLemmatizer\n",
    "example_words = ['history', 'formality', 'changes']\n",
    "for w in example_words:\n",
    "    print(lemmatizer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be77824",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:29:43.209935Z",
     "start_time": "2022-12-21T10:29:43.203504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'felicitous', 'happy', 'well-chosen', 'glad'}\n",
      "{'unhappy'}\n"
     ]
    }
   ],
   "source": [
    "# fing synonyms and antonyms using wordnet\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "synonyms = []   ## создаём пустой список всех синонимов\n",
    "antonyms =[]    ## создаём пустой список всех антонимов\n",
    "for syn in wordnet.synsets('happy'): ## загружаем слово\n",
    "    for i in syn.lemmas():         ## находим все соответств. леммы\n",
    "        synonyms.append(i.name())  ## добавляем все синонимы\n",
    "        if i.antonyms():\n",
    "            antonyms.append(i.antonyms()[0].name()) ## антонимы\n",
    "print(set(synonyms)) ## преобразуем их в множество \n",
    "                     ## уникальных значений\n",
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec79197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T10:35:13.398579Z",
     "start_time": "2022-12-21T10:35:09.652487Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kintal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('An', 'DT'), ('sincerity', 'NN'), ('so', 'RB'), ('extremity', 'NN'), ('he', 'PRP'), ('additions', 'VBZ'), ('.', '.'), ('Her', 'PRP$'), ('yet', 'RB'), ('there', 'EX'), ('truth', 'NN'), ('merit', 'NN'), ('.', '.'), ('Mrs', 'NNP'), ('all', 'DT'), ('projecting', 'VBG'), ('favourable', 'JJ'), ('now', 'RB'), ('unpleasing', 'VBG'), ('.', '.'), ('Son', 'NNP'), ('law', 'NN'), ('garden', 'NN'), ('chatty', 'JJ'), ('temper', 'NN'), ('.', '.'), ('Oh', 'UH'), ('children', 'NNS'), ('provided', 'VBD'), ('to', 'TO'), ('mr', 'VB'), ('elegance', 'NN'), ('marriage', 'NN'), ('strongly', 'RB'), ('.', '.'), ('Off', 'CC'), ('can', 'MD'), ('admiration', 'VB'), ('prosperous', 'JJ'), ('now', 'RB'), ('devonshire', 'VBP'), ('diminution', 'NN'), ('law', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# mark parts of speach\n",
    "import nltk\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "sample_text = '''\n",
    "An sincerity so extremity he additions. Her yet there truth merit. Mrs all projecting favourable now unpleasing. Son law garden chatty temper. Oh children provided to mr elegance marriage strongly. Off can admiration prosperous now devonshire diminution law.\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(sample_text)\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "411abdbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-21T13:40:29.719504Z",
     "start_time": "2022-12-21T13:40:29.695670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting text to vector for ml - example\n",
    "#\n",
    "# sent1 = he is a good boy\n",
    "# sent2 = she is a good girl\n",
    "# sent3 = boy and girl are good \n",
    "#         |\n",
    "#         |\n",
    "#   After removal of stopwords , lematization or stemming\n",
    "# sent1 = good boy\n",
    "# sent2 = good girl\n",
    "# sent3 = boy girl good  \n",
    "#         | ### Now we will calculate the frequency for each word by\n",
    "#         |     calculating the occurrence of each word\n",
    "# word  frequency\n",
    "# good     3\n",
    "# boy      2\n",
    "# girl     2\n",
    "#          | ## Then according to their occurrence we assign o or 1 \n",
    "#          |    according to their occurrence in the sentence\n",
    "#          | ## 1 for present and 0 fot not present\n",
    "#          f1  f2   f3\n",
    "#         girl good boy   \n",
    "# sent1    0    1    1     \n",
    "# sent2    1    0    1\n",
    "# sent3    1    1    1\n",
    "# ### After this we pass the vector form to machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text to vector for ml - code\n",
    "import pandas as pd\n",
    "\n",
    "sent = pd.DataFrame(['he is a good boy', 'she is a good girl', 'boy and girl are good'], columns=['text'])\n",
    "corpus = []\n",
    "for i in range(0, 3):\n",
    "    words = sent['text'][i]\n",
    "    words = word_tokenize(words)\n",
    "    texts = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    text = ' '.join(texts)\n",
    "    corpus.append(text)\n",
    "print(corpus)   #### очищенные данные\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer() ## создаём объект для CountVectorizer\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "X  ## векторная форма"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
